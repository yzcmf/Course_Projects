{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image Swap GAN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ZnA7glR0TfVo","colab_type":"code","outputId":"4a2ee441-37ce-4b37-a2b7-9ab632ca57df","executionInfo":{"status":"ok","timestamp":1556059698817,"user_tz":300,"elapsed":3091,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"cell_type":"code","source":["import platform\n","print(platform.python_version())\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["3.6.7\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 3683715674207936907, name: \"/device:XLA_CPU:0\"\n"," device_type: \"XLA_CPU\"\n"," memory_limit: 17179869184\n"," locality {\n"," }\n"," incarnation: 8137145047541298318\n"," physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n"," device_type: \"XLA_GPU\"\n"," memory_limit: 17179869184\n"," locality {\n"," }\n"," incarnation: 14454833893249691046\n"," physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n"," device_type: \"GPU\"\n"," memory_limit: 14800692839\n"," locality {\n","   bus_id: 1\n","   links {\n","   }\n"," }\n"," incarnation: 13401239721814633736\n"," physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"myaIIUzdTiGp","colab_type":"code","outputId":"a9fd1090-3476-4c69-f20a-c85b305e2280","executionInfo":{"status":"ok","timestamp":1556059698818,"user_tz":300,"elapsed":3072,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"v8R4B5Z0TiYl","colab_type":"code","outputId":"1d0b2ccd-7511-4e59-db9c-0262078b2c2c","executionInfo":{"status":"ok","timestamp":1556059700677,"user_tz":300,"elapsed":4905,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"cell_type":"code","source":["!ls '/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code'"],"execution_count":3,"outputs":[{"output_type":"stream","text":[" cage_0.mp4\t faceA_0\t\t    mtcnn_weights\n"," checkpoints\t faceB\t\t\t    preprocess.py\n"," checkpoints_0\t faceB_0\t\t    __pycache__\n"," converter\t'Image Swap GAN(0).ipynb'   train_samples_a.pkl\n"," detector\t'Image Swap GAN.ipynb'\t    train_samples_b.pkl\n"," dummy.mp4\t lib\t\t\t    trump_0.mp4\n"," faceA\t\t mtcnn_detect_face.py\t    umeyama.py\n"],"name":"stdout"}]},{"metadata":{"id":"TBhgUCrc-snU","colab_type":"code","outputId":"346003a9-9d5c-4a00-f345-9ce6b23b902b","executionInfo":{"status":"ok","timestamp":1556059700679,"user_tz":300,"elapsed":4877,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["%cd '/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code\n"],"name":"stdout"}]},{"metadata":{"id":"DaUKr21mJQGo","colab_type":"code","colab":{}},"cell_type":"code","source":["#!git clone https://github.com/shaoanlu/faceswap-GAN.git"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A9sUiCR8QwrM","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","\n","import pickle as pkl\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import random as rand\n","import glob\n","import os\n","import math\n","from PIL import Image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iy_aOFgT-PFR","colab_type":"code","outputId":"4082a12c-1347-4f8f-ecbc-34b40929f9fd","executionInfo":{"status":"ok","timestamp":1556059700685,"user_tz":300,"elapsed":4828,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["%pwd"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code'"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"ygOY1-rXHNJK","colab_type":"code","colab":{}},"cell_type":"code","source":["# from google.colab import files"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JbmDMVK1HNOu","colab_type":"code","colab":{}},"cell_type":"code","source":["# # Upload source video\n","# source_video = files.upload()\n","\n","# for fn_source_video, _ in source_video.items():\n","#     print(fn_source_video)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wlWOTCz3HPr-","colab_type":"code","colab":{}},"cell_type":"code","source":["# # Upload target video\n","# target_video = files.upload()\n","\n","# for fn_target_video, _ in target_video.items():\n","#     print(fn_target_video)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SzeIoDkODp4d","colab_type":"code","colab":{}},"cell_type":"code","source":["# %%capture\n","# !pip install moviepy\n","# !pip install keras_vggface\n","# import imageio\n","# imageio.plugins.ffmpeg.download()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"drOJWwZFDtB3","colab_type":"code","outputId":"5b4d0934-3f9d-4fbe-fafb-d1511854e286","executionInfo":{"status":"ok","timestamp":1556059701184,"user_tz":300,"elapsed":5247,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import keras.backend as K\n","from detector.face_detector import MTCNNFaceDetector\n","import glob\n","\n","from preprocess import preprocess_video"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"bKvxyNudDwtf","colab_type":"code","colab":{}},"cell_type":"code","source":["# fd = MTCNNFaceDetector(sess=K.get_session(), model_path=\"./mtcnn_weights/\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wIXrgN0lD0BW","colab_type":"code","colab":{}},"cell_type":"code","source":["# !mkdir -p faceA/rgb\n","# !mkdir -p faceA/binary_mask\n","# !mkdir -p faceB/rgb\n","# !mkdir -p faceB/binary_mask"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TmBYlTIXD578","colab_type":"code","colab":{}},"cell_type":"code","source":["fn_source_video = 'trump_0.mp4'\n","fn_target_video = 'cage_0.mp4'  \n","  \n","# save_interval1 = 5 # perform face detection every {save_interval} frames\n","# save_path = \"./faceA/\"\n","# preprocess_video(fn_source_video, fd, save_interval1, save_path)\n","# save_interval2 = 2\n","# save_path = \"./faceB/\"\n","# preprocess_video(fn_target_video, fd, save_interval2, save_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"usN5AsCDD_Sc","colab_type":"code","outputId":"c7eabacd-8534-4d98-80df-02286ea04076","executionInfo":{"status":"ok","timestamp":1556059701205,"user_tz":300,"elapsed":5195,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["print(str(len(glob.glob(\"faceA/rgb/*.*\"))) + \" face(s) extracted from source video: \" + fn_source_video + \".\")\n","print(str(len(glob.glob(\"faceB/rgb/*.*\"))) + \" face(s) extracted from target video: \" + fn_target_video + \".\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["708 face(s) extracted from source video: trump_0.mp4.\n","647 face(s) extracted from target video: cage_0.mp4.\n"],"name":"stdout"}]},{"metadata":{"id":"kHs94l-aT3xu","colab_type":"code","colab":{}},"cell_type":"code","source":["# Path to training images\n","img_dirA = './faceA/rgb'\n","img_dirB = './faceB/rgb'\n","img_dirA_bm_eyes = \"./faceA/binary_mask\"\n","img_dirB_bm_eyes = \"./faceB/binary_mask\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"s-ShQwG1BroB","colab_type":"code","outputId":"36b91c30-ac6a-4652-d8d7-54ad619f3c21","executionInfo":{"status":"ok","timestamp":1556059701210,"user_tz":300,"elapsed":5148,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# Get filenames\n","train_a = glob.glob(img_dirA+\"/*.*\")\n","train_b = glob.glob(img_dirB+\"/*.*\")\n","\n","train_AnB = train_a + train_b\n","\n","assert len(train_a), \"No image found in \" + str(img_dirA)\n","assert len(train_b), \"No image found in \" + str(img_dirB)\n","print (\"Number of images in folder A: \" + str(len(train_a)))\n","print (\"Number of images in folder B: \" + str(len(train_b)))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Number of images in folder A: 708\n","Number of images in folder B: 647\n"],"name":"stdout"}]},{"metadata":{"id":"M3cOXgCGhUQf","colab_type":"code","outputId":"5cf23264-cdcb-4bb9-dd55-6e2a575c63a2","executionInfo":{"status":"ok","timestamp":1556059701458,"user_tz":300,"elapsed":5377,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# train_A = train_a[:1000]\n","# train_B = train_b[:1000]\n","print (\"Number of images in training A: \" + str(len(train_a)))\n","print (\"Number of images in training B: \" + str(len(train_b)))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Number of images in training A: 708\n","Number of images in training B: 647\n"],"name":"stdout"}]},{"metadata":{"id":"AtZZA4k_YDiH","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_inputs(dim):\n","    inputs_real = tf.placeholder(tf.float32, (None, dim), name='input_real') \n","    inputs_fake = tf.placeholder(tf.float32, (None, dim), name='input_fake')\n","    \n","    return inputs_real, inputs_fake"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6Exf-6bPYDwJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def preprocess_image(original_image):\n","    maximum = np.max(original_image)\n","    minimum = np.min(original_image)\n","    # Scale to 0 and 1\n","    processed_image = (original_image - minimum)/(maximum-minimum)\n","    # Scale between -1 and 1\n","    processed_image = (processed_image*2) - 1\n","    return processed_image\n","    \n","def postprocess_image(original_image, processed_image):\n","    maximum = np.max(original_image)\n","    minimum = np.min(original_image)\n","    proc_max = np.max(processed_image)\n","    proc_min = np.min(processed_image)\n","    # Scale between -1 and 1\n","    output_image = (processed_image - proc_min)/(proc_max - proc_min)\n","    # Scale to 0 and 1\n","    output_image = (output_image * (maximum-minimum)) + minimum\n","    output_image = output_image.astype(int)\n","    return output_image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dxNftATdYLk6","colab_type":"code","colab":{}},"cell_type":"code","source":["# def encoder(image_input, latent_size, n_units=128, reuse=False, alpha=0.01, scope='encoder'):\n","#     with tf.variable_scope(scope, reuse=reuse):\n","        \n","#         # Hidden layer\n","#         hidden = tf.layers.dense(image_input, n_units, activation=None)\n","#         print('hidden is:', hidden)\n","        \n","#         # Leaky ReLU\n","#         hidden = tf.maximum(hidden, alpha*hidden)\n","#         print('hidden is:', hidden)\n","        \n","#         # Logits and tanh output\n","#         logits = tf.layers.dense(hidden, latent_size, activation=None)\n","#         print('logits is:', logits)\n","#         latents = tf.nn.tanh(logits)\n","#         print('latents out is:', latents)\n","#         return latents\n","    \n","# def decoder(latents, out_dim, n_units=128, reuse=False, alpha=0.01, scope='decoder'):\n","#     with tf.variable_scope(scope, reuse=reuse):\n","        \n","#         # Hidden layer\n","#         hidden = tf.layers.dense(latents, n_units, activation=None)\n","#         print('hidden is:', hidden)\n","        \n","#         # Leaky ReLU\n","#         hidden = tf.maximum(hidden, alpha*hidden)\n","#         print('hidden is:', hidden)\n","\n","#         # Logits and tanh output\n","#         logits = tf.layers.dense(hidden, out_dim, activation=None)\n","#         print('logits is:', logits)\n","#         out = tf.nn.tanh(logits)\n","#         print('decoder out is:', out)\n","        \n","#         return out\n","    \n","# def discriminator(inputs, n_units=128, reuse=False, alpha=0.01, scope='discriminator'):\n","#     with tf.variable_scope(scope, reuse=reuse):\n","        \n","#         # Hidden layer\n","#         h1 = tf.layers.dense(inputs, n_units, activation=None)\n","#         print('hidden is:', h1)\n","        \n","#         # Leaky ReLU\n","#         h1 = tf.maximum(alpha * h1, h1)\n","#         print('hidden is:', h1)\n","\n","#         logits = tf.layers.dense(h1, 1, activation=None)\n","#         print('logits is:', logits)\n","#         out = tf.sigmoid(logits)\n","#         print('discriminator out is:', out)\n","        \n","#         return out, logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EjXfAhOOAYYX","colab_type":"code","colab":{}},"cell_type":"code","source":["# losses da,ga,db,gb saved as: (0.9020894, 3.3791414e-06, 0.36076805, 0.23865208)\n","# batch_a (100, 12288)\n","# batch_b (100, 12288)\n","# encoder_val Tensor(\"generator_1/Tanh:0\", shape=(?, 100), dtype=float32)\n","# decoder_a_val Tensor(\"dec_a_1/Tanh:0\", shape=(?, 12288), dtype=float32)\n","# feed_dict_decoder {<tf.Tensor 'input_fake:0' shape=(?, 12288) dtype=float32>: array([[-0.20784314, -0.36470588, -0.56078431, ..., -0.71764706,\n","#         -0.80392157, -0.85098039],\n","#        [-0.56862745, -0.62352941, -0.68627451, ...,  0.23921569,\n","#         -0.04313725, -0.16862745],\n","#        [-0.56078431, -0.63137255, -0.68627451, ...,  0.08235294,\n","#         -0.29411765, -0.4745098 ],\n","#        ...,\n","#        [-0.56078431, -0.64705882, -0.69411765, ...,  0.09803922,\n","#         -0.34117647, -0.4745098 ],\n","#        [-0.56078431, -0.61568627, -0.67843137, ...,  0.09803922,\n","#         -0.30980392, -0.4745098 ],\n","#        [-0.56078431, -0.64705882, -0.69411765, ...,  0.56862745,\n","#          0.27843137,  0.12941176]])}\n","# samples_a saved as: []\n","# samples_b saved as: []\n","\n","# losses da,ga,db,gb saved as: (0.5098937, 11.09968, 0.43515426, 14.043441)\n","# batch_a (100, 12288)\n","# batch_b (100, 12288)\n","# inside encoder\n","# input_ shape is: (?, 64, 64, 3)\n","# x shape is: (?, 64, 64, 3)\n","# x shape is: (?, 32, 32, 128)\n","# x shape is: (?, 16, 16, 256)\n","# x shape is: (?, 8, 8, 512)\n","# x shape is: (?, 4, 4, 1024)\n","# x shape is: (?, 1024)\n","# x shape is: (?, 16384)\n","# x shape is: (?, 4, 4, 1024)\n","# x shape is: (?, 8, 8, 512)\n","# encoder_val Tensor(\"generator_1/pixel_shuffler_8/Reshape_1:0\", shape=(?, 8, 8, 512), dtype=float32)\n","# inside decoder\n","# input_ shape is: (?, 8, 8, 512)\n","# x shape is: (?, 8, 8, 512)\n","# x shape is: (?, 16, 16, 256)\n","# x shape is: (?, 32, 32, 128)\n","# x shape is: (?, 64, 64, 64)\n","# x shape is: (?, 64, 64, 3)\n","# will leave the decoder, x is :  Tensor(\"dec_a_1/conv2d_22/Sigmoid:0\", shape=(?, 64, 64, 3), dtype=float32)\n","# decoder_a_val Tensor(\"dec_a_1/conv2d_22/Sigmoid:0\", shape=(?, 64, 64, 3), dtype=float32)\n","# feed_dict_decoder {<tf.Tensor 'input_fake:0' shape=(?, 12288) dtype=float32>: array([[-0.56078431, -0.63137255, -0.68627451, ...,  0.23921569,\n","#         -0.09803922, -0.23137255],\n","#        [-0.56078431, -0.64705882, -0.69411765, ...,  0.17647059,\n","#         -0.11372549, -0.23921569],\n","#        [-0.56078431, -0.64705882, -0.69411765, ...,  0.11372549,\n","#         -0.30196078, -0.44313725],\n","#        ...,\n","#        [-0.56078431, -0.64705882, -0.69411765, ...,  0.14509804,\n","#         -0.04313725, -0.12156863],\n","#        [-0.67058824, -0.77254902, -0.90588235, ..., -0.37254902,\n","#         -0.54509804, -0.65490196],\n","#        [ 0.20784314,  0.01960784, -0.2       , ..., -0.70980392,\n","#         -0.78823529, -0.88235294]])}\n","\n","\n","# losses da,ga,db,gb saved as: (0.5789854, 3.9114652, 1.0304557, 472.15677)\n","# batch_a (100, 12288)\n","# batch_b (100, 12288)\n","# inside encoder\n","# input_ shape is: (?, 64, 64, 3)\n","# x shape is: (?, 64, 64, 3)\n","# x shape is: (?, 32, 32, 128)\n","# x shape is: (?, 16, 16, 256)\n","# x shape is: (?, 8, 8, 512)\n","# x shape is: (?, 4, 4, 1024)\n","# x shape is: (?, 1024)\n","# x shape is: (?, 16384)\n","# x shape is: (?, 4, 4, 1024)\n","# x shape is: (?, 8, 8, 512)\n","# encoder_val Tensor(\"generator_1/pixel_shuffler_8/Reshape_1:0\", shape=(?, 8, 8, 512), dtype=float32)\n","# inside decoder\n","# input_ shape is: (?, 8, 8, 512)\n","# x shape is: (?, 8, 8, 512)\n","# x shape is: (?, 16, 16, 256)\n","# x shape is: (?, 32, 32, 128)\n","# x shape is: (?, 64, 64, 64)\n","# x shape is: (?, 64, 64, 3)\n","# x shape is: (?, 12288)\n","# x shape is: (?, 12288)\n","# x shape is: (?, 64, 64, 3)\n","# decoder_a_val Tensor(\"dec_a_1/reshape_5/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32)\n","# feed_dict_decoder {<tf.Tensor 'input_fake:0' shape=(?, 12288) dtype=float32>: array([[-0.56078431, -0.63137255, -0.68627451, ...,  0.55294118,\n","#          0.20784314,  0.01176471],\n","#        [-0.56078431, -0.64705882, -0.69411765, ...,  0.12941176,\n","#         -0.30980392, -0.44313725],\n","#        [-0.57647059, -0.64705882, -0.70196078, ...,  0.09019608,\n","#         -0.28627451, -0.45882353],\n","#        ...,\n","#        [-0.56862745, -0.65490196, -0.68627451, ...,  0.08235294,\n","#         -0.29411765, -0.45098039],\n","#        [-0.56078431, -0.64705882, -0.69411765, ...,  0.11372549,\n","#         -0.30196078, -0.44313725],\n","#        [-0.56078431, -0.64705882, -0.69411765, ...,  0.1372549 ,\n","#         -0.30980392, -0.44313725]])}\n","\n","# FailedPreconditionError: Attempting to use uninitialized value dec_a_1/dense_7/bias\n","# \t [[{{node dec_a_1/dense_7/bias/read}}]]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JYUUYEmjK_DZ","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Input, Dense, Flatten, Reshape\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import Conv2D\n","from lib.PixelShuffler import PixelShuffler"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BhqZ57cYLaR4","colab_type":"code","colab":{}},"cell_type":"code","source":["IMAGE_SHAPE = (64, 64, 3)\n","ENCODER_DIM = 1024"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ec_eBl2VKyVF","colab_type":"code","colab":{}},"cell_type":"code","source":["def conv_block(x,filters):\n","            x = Conv2D(filters, kernel_size=5, strides=2, padding='same')(x)\n","            x = LeakyReLU(0.1)(x)\n","            return x\n","      \n","def upscale_block(x,filters):\n","            x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)\n","            x = LeakyReLU(0.1)(x) # 使用 LeakyReLU 激活函数\n","            x = PixelShuffler()(x) # 将filter的大小变为原来的1/4，让高和宽变为原来的两倍\n","            return x\n","def lrelu(x):\n","    return tf.maximum(x, tf.multiply(x, 0.2))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PFBdX7vRR5j3","colab_type":"code","colab":{}},"cell_type":"code","source":["# def encoder(image_input, latent_size, n_units=128, reuse=False, alpha=0.01, scope='encoder'):\n","#     with tf.variable_scope(scope, reuse=reuse):\n","        \n","#         # Hidden layer\n","#         hidden = tf.layers.dense(image_input, n_units, activation=None)\n","#         print('hidden is:', hidden)\n","        \n","#         # Leaky ReLU\n","#         hidden = tf.maximum(hidden, alpha*hidden)\n","#         print('hidden is:', hidden)\n","        \n","#         # Logits and tanh output\n","#         logits = tf.layers.dense(hidden, latent_size, activation=None)\n","#         print('logits is:', logits)\n","#         latents = tf.nn.tanh(logits)\n","#         print('latents out is:', latents)\n","#         return latents\n","    \n","# def decoder(latents, out_dim, n_units=128, reuse=False, alpha=0.01, scope='decoder'):\n","#     with tf.variable_scope(scope, reuse=reuse):\n","        \n","#         # Hidden layer\n","#         hidden = tf.layers.dense(latents, n_units, activation=None)\n","#         print('hidden is:', hidden)\n","        \n","#         # Leaky ReLU\n","#         hidden = tf.maximum(hidden, alpha*hidden)\n","#         print('hidden is:', hidden)\n","\n","#         # Logits and tanh output\n","#         logits = tf.layers.dense(hidden, out_dim, activation=None)\n","#         print('logits is:', logits)\n","#         out = tf.nn.tanh(logits)\n","#         print('decoder out is:', out)\n","        \n","#         return out\n","    \n","# def discriminator(inputs, n_units=128, reuse=False, alpha=0.01, scope='discriminator'):\n","#     with tf.variable_scope(scope, reuse=reuse):\n","        \n","#         # Hidden layer\n","#         h1 = tf.layers.dense(inputs, n_units, activation=None)\n","#         print('hidden is:', h1)\n","        \n","#         # Leaky ReLU\n","#         h1 = tf.maximum(alpha * h1, h1)\n","#         print('hidden is:', h1)\n","\n","#         logits = tf.layers.dense(h1, 1, activation=None)\n","#         print('logits is:', logits)\n","#         out = tf.sigmoid(logits)\n","#         print('discriminator out is:', out)\n","        \n","#         return out, logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nH5SsTKogjDW","colab_type":"code","colab":{}},"cell_type":"code","source":["def encoder(image_input, latent_size, n_units=128, reuse=False, alpha=0.01, scope='encoder'):\n","    with tf.variable_scope(scope, reuse=reuse):\n","        print('inside encoder')\n","#         input_ = Input(shape=IMAGE_SHAPE)\n","        input_ = tf.reshape(image_input, shape=[-1, 64, 64, 3])\n","        print('input_ is:', input_)\n","        x = input_\n","        print('x is:', x)\n","        x = conv_block(x,128)\n","        print('x is:', x)\n","        x = conv_block(x,256)\n","        print('x is:', x)\n","        x = conv_block(x,512)\n","        print('x is:', x)\n","        x = conv_block(x,1024)\n","        print('x is:', x)\n","        x = Dense(ENCODER_DIM)(Flatten()(x))\n","        print('x is:', x)\n","        x = Dense(4 * 4 * 1024)(x)\n","        print('x is:', x)\n","        x = Reshape((4, 4, 1024))(x)\n","        print('x is:', x)\n","        x = upscale_block(x,512)\n","        print('x is:', x)\n","        \n","        return x\n","        \n","#         # Hidden layer\n","#         hidden = tf.layers.dense(image_input, n_units, activation=None) # 160 -> 128\n","        \n","#         # Leaky ReLU\n","#         hidden = tf.maximum(hidden, alpha*hidden)\n","        \n","#         # Logits and tanh output\n","#         x = tf.layers.dense(x, latent_size, activation=None) # 128 -> 100\n","#         latents = tf.nn.tanh(x)\n","#         return latents\n","    \n","def decoder(latents, out_dim, n_units=128, reuse=False, alpha=0.01, scope='decoder'):\n","    with tf.variable_scope(scope, reuse=reuse):\n","        print('inside decoder')\n","        input_ = Input(shape=(8, 8, 512))\n","        input_ = tf.reshape(latents, shape=[-1, 8, 8, 512])\n","        print('input_ is:', input_)\n","        x = input_\n","        print('x is:', x)\n","#         x = latents\n","        x = upscale_block(x,256)\n","        print('x is:', x)\n","        \n","#         # Hidden layer\n","#         x = tf.layers.dense(latents, n_units, activation=None) # 100 -> 128\n","#         print('x shape is:', x.shape)\n","        x = upscale_block(x,128)\n","        print('x is:', x)\n","        x = upscale_block(x,64)\n","        print('x is:', x)\n","        x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)\n","        print('x is:', x)\n","\n","\n","    \n","        x = Dense(out_dim)(Flatten()(x))\n","        print('x is:', x)\n","        x = Dense(64 * 64 * 3)(x)\n","        print('x shape is:', x.shape)\n","#         x = Reshape((64, 64, 3))(x)\n","#         print('x shape is:', x.shape)\n","        \n","#         x = tf.reshape(latents, shape=[-1, 12288])\n","#         print('will leave the decoder, x shape is : ' , x)\n","\n","#         logits = tf.layers.dense(x, out_dim, activation=None) # 128 -> out_dim = 160 || 64\n","        x = tf.nn.tanh(x)\n","      \n","        return x\n","       \n","#         # Leaky ReLU\n","#         hidden = tf.maximum(hidden, alpha*hidden)\n","\n","#         Logits and tanh output\n","#         logits = tf.layers.dense(x, out_dim, activation=None) # 128 -> out_dim = 160 || 64\n","#         tanh_out = tf.nn.tanh(logits)\n","        \n","#         return tanh_out\n","    \n","def discriminator(inputs, n_units=128, reuse=False, alpha=0.01, scope='discriminator'):\n","    print('inside discriminator model function')\n","    activation = lrelu\n","    with tf.variable_scope(scope, reuse=reuse):\n","        \n","#         # Hidden layer\n","#         print('inputs is: ' , inputs) \n","#         print('n_units is: ' , n_units)\n","        \n","#         hidden = tf.layers.dense(inputs, n_units, activation=None) # out_dim = 160 || 64 -> 128 \n","#         print(hidden)\n","#         # Leaky ReLU\n","#         hidden = tf.maximum(hidden, alpha*hidden)\n","\n","        x = tf.reshape(inputs, shape=[-1, 64, 64, 3])\n","        print('x is:', x)\n","        x = tf.layers.conv2d(x, kernel_size=5, filters=64, strides=2, padding='same', activation=activation)\n","        print('x is:', x)\n","        x = tf.layers.dropout(x,0.6)\n","        print('x is:', x)\n","        x = tf.layers.conv2d(x, kernel_size=5, filters=64, strides=1, padding='same', activation=activation)\n","        print('x is:', x)\n","        x = tf.layers.dropout(x,0.6)\n","        print('x is:', x)\n","        x = tf.layers.conv2d(x, kernel_size=5, filters=64, strides=1, padding='same', activation=activation)\n","        print('x is:', x)\n","        x = tf.layers.dropout(x, 0.6)\n","        print('x is:', x)\n","        x = tf.contrib.layers.flatten(x)\n","        print('x is:', x)\n","        x = tf.layers.dense(x, units=128, activation=activation)\n","        print('x is:', x)\n","#         sigmoid_out = tf.layers.dense(x, units=1, activation=tf.nn.sigmoid)\n","\n","        # Sigmoid output and Logits \n","        logits = tf.layers.dense(x, 1, activation=None) # 128 -> 1\n","        print('logits:', logits)\n","        sigmoid_out = tf.sigmoid(logits)\n","        print('sigmoid_out:', sigmoid_out )\n","        \n","        return sigmoid_out, logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OxiDLnu3QwrT","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(imageset_a, imageset_b, input_size, latent_size, g_hidden_size, d_hidden_size, \n","          alpha, smooth, learning_rate, batch_size, epochs, reuse):\n","    # Build Network\n","    tf.reset_default_graph()\n","    \n","    # Necessary Scopes\n","    generator_encoder_scope = 'generator'\n","    generator_decoder_a_scope = 'dec_a'\n","    generator_decoder_b_scope = 'dec_b'\n","    discriminator_a_scope = 'discriminator_a'\n","    discriminator_b_scope = 'discriminator_b'\n","    \n","#     print('default graph and scope seting finish.')\n","#     print(\"imageset a shape is: \" + str(imageset_a.shape) + \";imageset b shape is: \" + str(imageset_b.shape))\n","    \n","    \n","    with tf.device('/device:GPU:0'):\n","      # Create our input placeholders\n","      input_real, input_fake = model_inputs(input_size)\n","      \n","      # Build the model\n","      # 1.g_model is the generator output\n","      g_model_enc = encoder(input_fake, latent_size, n_units=g_hidden_size, \n","                            alpha=alpha, scope=generator_encoder_scope)\n","      g_model_dec_a = decoder(g_model_enc, input_size, n_units=g_hidden_size, \n","                              alpha=alpha, scope=generator_decoder_a_scope)\n","      g_model_dec_b = decoder(g_model_enc, input_size, n_units=g_hidden_size, \n","                              alpha=alpha, scope=generator_decoder_b_scope)\n","      \n","#       print('g_model seting finish.')\n","      \n","      # 2.d_model is the discrimator output\n","      d_model_real_a, d_logits_real_a = discriminator(input_real, n_units=d_hidden_size, \n","                                                      alpha=alpha, scope=discriminator_a_scope)\n","      d_model_fake_a, d_logits_fake_a = discriminator(g_model_dec_a, reuse=True, n_units=d_hidden_size, \n","                                                      alpha=alpha, scope=discriminator_a_scope)\n","      d_model_real_b, d_logits_real_b = discriminator(input_real, n_units=d_hidden_size, \n","                                                      alpha=alpha, scope=discriminator_b_scope)\n","      d_model_fake_b, d_logits_fake_b = discriminator(g_model_dec_b, reuse=True, n_units=d_hidden_size, \n","                                                      alpha=alpha, scope=discriminator_b_scope)\n","      \n","#       print('d_model seting finish.')\n","      \n","      # Define the loss functions, get the trainable_variables, split into G and D parts\n","      d_loss_real_a = tf.reduce_mean(\n","                        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real_a, \n","                                                                labels=tf.ones_like(d_logits_real_a) * (1 - smooth)))\n","      d_loss_fake_a = tf.reduce_mean(\n","                        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake_a, \n","                                                                labels=tf.zeros_like(d_logits_fake_a)))\n","      d_loss_a = d_loss_real_a + d_loss_fake_a\n","      g_loss_a = tf.reduce_mean(\n","                   tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake_a,\n","                                                           labels=tf.ones_like(d_logits_fake_a)))\n","      d_loss_real_b = tf.reduce_mean(\n","                        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real_b, \n","                                                                labels=tf.ones_like(d_logits_real_b) * (1 - smooth)))\n","      d_loss_fake_b = tf.reduce_mean(\n","                        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake_b, \n","                                                                labels=tf.zeros_like(d_logits_fake_b)))\n","      d_loss_b = d_loss_real_b + d_loss_fake_b\n","      g_loss_b = tf.reduce_mean(\n","                   tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake_b,\n","                                                           labels=tf.ones_like(d_logits_fake_b)))\n","      \n","#       print('loss seting finish.')\n","      \n","      t_vars = tf.trainable_variables()\n","      g_vars = [var for var in t_vars if var.name.startswith(generator_encoder_scope)]\n","      g_vars_a = [var for var in t_vars if var.name.startswith(generator_decoder_a_scope)]\n","      d_vars_a = [var for var in t_vars if var.name.startswith(discriminator_a_scope)]\n","      g_vars_b = [var for var in t_vars if var.name.startswith(generator_decoder_b_scope)]\n","      d_vars_b = [var for var in t_vars if var.name.startswith(discriminator_b_scope)]\n","      \n","      d_train_opt_a = tf.train.AdamOptimizer(learning_rate).minimize(d_loss_a, var_list=d_vars_a)\n","      g_train_opt_a = tf.train.AdamOptimizer(learning_rate).minimize(g_loss_a, var_list=g_vars + g_vars_a)\n","      d_train_opt_b = tf.train.AdamOptimizer(learning_rate).minimize(d_loss_b, var_list=d_vars_b)\n","      g_train_opt_b = tf.train.AdamOptimizer(learning_rate).minimize(g_loss_b, var_list=g_vars + g_vars_b)\n","      \n","#       print('Adam opt finish.')\n","    \n","    samples_a = []\n","    samples_b = []\n","    losses = []\n","    # Only save generator variables\n","    saver = tf.train.Saver(var_list=g_vars + g_vars_a + g_vars_b + d_vars_a+ d_vars_b)\n","    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","#         sess.run(tf.global_variables_initializer())\n","#         sess.run(tf.local_variables_initializer())\n","        init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())\n","        sess.run(init_op)\n","        if reuse:\n","            print(\"Reloading from checkpoint\")\n","            saver = tf.train.import_meta_graph('/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code/checkpoints/GAN_b.ckpt.meta')\n","            saver.restore(sess, '/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code/checkpoints/GAN_b.ckpt')\n","        for e in range(epochs):\n","          \n","            print('Epochs begin at', e)\n","            \n","            \n","            np.random.shuffle(imageset_a)\n","            np.random.shuffle(imageset_b)\n","            \n","#             print(\"imageset a shape is: \" + str(imageset_a.shape) + \";imageset b shape is: \" + str(imageset_b.shape))\n","            \n","            def epoch_optimizer(imageset_real, imageset_fake, train_opt, loss):               \n","                for ii in range(min(imageset_real.shape[0]//batch_size, imageset_fake.shape[0]//batch_size)):\n","                    \n","#                     print('epoch_optimizer begin at', ii  )\n","                    \n","                    batch_real = np.array(imageset_real[ii*batch_size:(ii+1)*batch_size])\n","                    batch_fake = np.array(imageset_fake[ii*batch_size:(ii+1)*batch_size])\n","                    \n","#                     print('epoch_optimizer 1:', ii)\n","#                     print(\"batch_real shape is: \" + str(batch_real.shape) + \";batch_fake shape is: \" + str(batch_fake.shape))\n","                    \n","                    \n","                    # Get images, reshape and rescale to pass to D\n","                    batch_images_real = batch_real.reshape((batch_size, input_size))\n","                    \n","#                     print('epoch_optimizer 2:', ii)\n","#                     print(\"batch_images_real shape is: \" + str(batch_images_real.shape))\n","                    \n","                    \n","                    # Adversarial images for G\n","                    batch_images_fake = batch_fake.reshape((batch_size, input_size))\n","                    \n","#                     print('epoch_optimizer 3:', ii)\n","#                     print(\"batch_images_fake shape is: \" + str(batch_images_fake.shape))\n","#                     print(\"train_opt:\", train_opt)\n","#                     print(\"feed dict:\", {input_real: batch_images_real,input_fake: batch_images_fake})\n","                    \n","                    # Run optimizers\n","                    _ = sess.run(train_opt, feed_dict={input_real: batch_images_real, \n","                                                         input_fake: batch_images_fake})\n","                    \n","                    \n","#                     print('train_opt: ', train_opt)\n","#                     feed_dict_= {input_real: batch_images_real, input_fake: batch_images_fake}\n","#                     print('feed_dict_: ', feed_dict_)\n","                                 \n","#                     print('epoch_optimizer 4:', ii)\n","                      \n","                # At the end of each epoch, get the losses and print them out\n","                train_loss = sess.run(loss, {input_fake: imageset_fake, input_real: imageset_real})\n","                \n","#                 print('loss now: ', loss)\n","#                 feed_dict_tloss = {input_real: batch_images_real, input_fake: batch_images_fake}\n","#                 print('feed_dict_tloss: ', feed_dict_tloss)\n","                \n","#                 print('epoch_optimizer 5:', ii)\n","                  \n","                return train_opt, train_loss\n","            \n","#             print('start epoch_optimizer')\n","            \n","            d_train_opt_a, train_loss_d_a = epoch_optimizer(imageset_a, imageset_b, d_train_opt_a, d_loss_a)\n","            \n","#             print('d_train_opt_a, train_loss_d_a finish.')\n","            \n","            d_train_opt_b, train_loss_d_b = epoch_optimizer(imageset_b, imageset_a, d_train_opt_b, d_loss_b)\n","            \n","#             print('d_train_opt_b, train_loss_d_b finish.')\n","              \n","            g_train_opt_a, train_loss_g_a = epoch_optimizer(imageset_a, imageset_b, g_train_opt_a, g_loss_a)\n","            \n","#             print('g_train_opt_a, train_loss_g_a finish.')\n","            \n","            \n","            g_train_opt_b, train_loss_g_b = epoch_optimizer(imageset_b, imageset_a, g_train_opt_b, g_loss_b)\n","            \n","#             print('g_train_opt_b, train_loss_g_b finish.')\n","            \n","#             print('finish epoch_optimizer')\n","            \n","            # Statements to print each epoch\n","               \n","\n","#             if (e+1)%50 == 0 or (e == 0):\n","#                 print(\"Epoch {}/{}...\".format(e+1, epochs),\n","#                       \"Discriminator_a Loss: {:.4f}...\".format(train_loss_d_a),\n","#                       \"Generator_a Loss: {:.4f}\".format(train_loss_g_a),\n","#                       \"Discriminator_b Loss: {:.4f}...\".format(train_loss_d_b),\n","#                       \"Generator_b Loss: {:.4f}\".format(train_loss_g_b))\n","            \n","            # Save losses to view after training\n","            losses = (train_loss_d_a, train_loss_g_a, train_loss_d_b, train_loss_g_b)\n","            \n","            print('losses da,ga,db,gb saved as:', losses)\n","            \n","            # Sample from generator as we're training for viewing afterwards\n","            if (e+1) == epochs:\n","                \n","                batch_a = np.array(imageset_a[0:batch_size])\n","                batch_b = np.array(imageset_b[0:batch_size])\n","                \n","                print('batch_a', batch_a.shape)\n","                print('batch_b', batch_b.shape)\n","                \n","#                 sess.run(tf.local_variables_initializer())\n","                encoder_val = encoder(input_fake, latent_size, n_units=g_hidden_size, alpha=alpha, reuse=True, \n","                                scope=generator_encoder_scope)\n","                \n","                print('encoder_val', encoder_val)\n","                \n","#                 sess.run(tf.local_variables_initializer())\n","                decoder_a_val = decoder(encoder_val, input_size, n_units=g_hidden_size, \n","                                         reuse=True, scope=generator_decoder_a_scope)\n","                \n","                print('decoder_a_val', decoder_a_val)\n","                feed_dict_decoder= {input_fake: batch_b}\n","                \n","                print('feed_dict_decoder', feed_dict_decoder)\n","                \n","                \n","#                 sess.run(tf.local_variables_initializer())\n","                gen_samples_a = sess.run(decoder_a_val, feed_dict_decoder)\n","                \n","                print('samples_a saved as:', samples_a)\n","                \n","                gen_samples_b = sess.run(\n","                               decoder(encoder(input_fake, latent_size, n_units=g_hidden_size, alpha=alpha, reuse=True, \n","                                scope=generator_encoder_scope), input_size, n_units=g_hidden_size, reuse=True,\n","                                scope=generator_decoder_b_scope), feed_dict={input_fake: batch_a})\n","                \n","                print('samples_b saved as:', samples_b)\n","                \n","                samples_a = gen_samples_a\n","                samples_b = gen_samples_b\n","\n","            if e != 0 and e % 5 == 0:\n","                save_loc = saver.save(sess, '/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code/checkpoints/GAN_b.ckpt')\n","        \n","        # This change will solve the memory problem\n","        #save_loc = saver.save(sess, '/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code/checkpoints/GAN.ckpt')\n","    \n","    # Save training generator samples\n","    with open('/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code/train_samples_a.pkl', 'wb') as f:\n","        pkl.dump(samples_a, f)\n","    with open('/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code/train_samples_b.pkl', 'wb') as f:\n","        pkl.dump(samples_b, f)\n","        \n","    return samples_a, samples_b, losses, g_vars, g_vars_a, g_vars_b"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fCjLtqLkQwrd","colab_type":"code","outputId":"d3530240-6bc9-498b-b167-19474e1dbaa2","executionInfo":{"status":"error","timestamp":1556059771092,"user_tz":300,"elapsed":74738,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":5254}},"cell_type":"code","source":["# SETUP IMAGES HERE, Images is array [image number, size of image]\n","\n","\n","img_dirA = 'faceA/rgb'\n","img_dirB = 'faceB/rgb'\n","train_A = glob.glob(img_dirA+\"/*.*\")\n","train_B = glob.glob(img_dirB+\"/*.*\")\n","# images_a = np.array([np.array(Image.open(fname).resize((160,160))) for fname in train_a])\n","# images_b = np.array([np.array(Image.open(fname).resize((160,160))) for fname in train_b])\n","\n","images_a = np.array([np.array(Image.open(fname).resize((64,64))) for fname in train_a])\n","images_b = np.array([np.array(Image.open(fname).resize((64,64))) for fname in train_b])\n","\n","print(\"images a shape is: \" + str(images_a.shape) + \";images b shape is: \" + str(images_b.shape))\n","\n","\n","flattendedimages_a = np.array(images_a).reshape((images_a.shape[0], -1))\n","flattendedimages_b = np.array(images_b).reshape((images_b.shape[0], -1))\n","# Size of input images to discriminators (SIZE OF IMAGE)\n","assert(flattendedimages_a.shape[1] == flattendedimages_b.shape[1])\n","input_size = flattendedimages_a.shape[1]\n","\n","print(\"input size is: \" + str(flattendedimages_a.shape[1]))\n","\n","\n","# Scale images to fit functions\n","scaledimages_a = preprocess_image(flattendedimages_a)\n","scaledimages_b = preprocess_image(flattendedimages_b)\n","\n","# Size of latent vector to generator\n","latent_size = 100\n","# Sizes of hidden layers in generator and discriminator\n","g_hidden_size = int(math.pow(2, int(math.log(input_size, 2)) - int(math.log(latent_size * 2, 2))))\n","d_hidden_size = g_hidden_size\n","# Leak factor for leaky ReLU\n","alpha = 0.01\n","# Smoothing \n","smooth = 0.1 # used for loss defination\n","# Optimizers\n","learning_rate = 0.001\n","# Step size\n","batch_size = 100\n","# Training Rounds\n","epochs = 2 # Having run around 200 + 1500 + 2500 + 800 + 2100 + 2900\n","# If we want to reuse old model\n","reuse = False\n","\n","samples_a, samples_b, losses, g_vars, g_vars_a, g_vars_b = train(scaledimages_a, scaledimages_b, input_size, \n","                latent_size, g_hidden_size, d_hidden_size, alpha, smooth, learning_rate, batch_size, epochs, reuse)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["images a shape is: (708, 64, 64, 3);images b shape is: (647, 64, 64, 3)\n","input size is: 12288\n","inside encoder\n","input_ is: Tensor(\"generator/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"generator/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32, device=/device:GPU:0)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","x is: Tensor(\"generator/leaky_re_lu_1/LeakyRelu:0\", shape=(?, 32, 32, 128), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"generator/leaky_re_lu_2/LeakyRelu:0\", shape=(?, 16, 16, 256), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"generator/leaky_re_lu_3/LeakyRelu:0\", shape=(?, 8, 8, 512), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"generator/leaky_re_lu_4/LeakyRelu:0\", shape=(?, 4, 4, 1024), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"generator/dense_1/BiasAdd:0\", shape=(?, 1024), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"generator/dense_2/BiasAdd:0\", shape=(?, 16384), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"generator/reshape_1/Reshape:0\", shape=(?, 4, 4, 1024), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"generator/pixel_shuffler_1/Reshape_1:0\", shape=(?, 8, 8, 512), dtype=float32, device=/device:GPU:0)\n","inside decoder\n","input_ is: Tensor(\"dec_a/Reshape:0\", shape=(?, 8, 8, 512), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_a/Reshape:0\", shape=(?, 8, 8, 512), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_a/pixel_shuffler_2/Reshape_1:0\", shape=(?, 16, 16, 256), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_a/pixel_shuffler_3/Reshape_1:0\", shape=(?, 32, 32, 128), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_a/pixel_shuffler_4/Reshape_1:0\", shape=(?, 64, 64, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_a/conv2d_9/Sigmoid:0\", shape=(?, 64, 64, 3), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_a/dense_3/BiasAdd:0\", shape=(?, 12288), dtype=float32, device=/device:GPU:0)\n","x shape is: (?, 12288)\n","inside decoder\n","input_ is: Tensor(\"dec_b/Reshape:0\", shape=(?, 8, 8, 512), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_b/Reshape:0\", shape=(?, 8, 8, 512), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_b/pixel_shuffler_5/Reshape_1:0\", shape=(?, 16, 16, 256), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_b/pixel_shuffler_6/Reshape_1:0\", shape=(?, 32, 32, 128), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_b/pixel_shuffler_7/Reshape_1:0\", shape=(?, 64, 64, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_b/conv2d_13/Sigmoid:0\", shape=(?, 64, 64, 3), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"dec_b/dense_5/BiasAdd:0\", shape=(?, 12288), dtype=float32, device=/device:GPU:0)\n","x shape is: (?, 12288)\n","inside discriminator model function\n","x is: Tensor(\"discriminator_a/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32, device=/device:GPU:0)\n","WARNING:tensorflow:From <ipython-input-28-51eb2be9566a>:103: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.conv2d instead.\n","x is: Tensor(\"discriminator_a/conv2d/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","WARNING:tensorflow:From <ipython-input-28-51eb2be9566a>:105: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dropout instead.\n","x is: Tensor(\"discriminator_a/dropout/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a/conv2d_1/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a/dropout_1/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a/conv2d_2/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a/dropout_2/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","x is: Tensor(\"discriminator_a/Flatten/flatten/Reshape:0\", shape=(?, 65536), dtype=float32, device=/device:GPU:0)\n","WARNING:tensorflow:From <ipython-input-28-51eb2be9566a>:117: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","x is: Tensor(\"discriminator_a/dense/Maximum:0\", shape=(?, 128), dtype=float32, device=/device:GPU:0)\n","logits: Tensor(\"discriminator_a/dense_1/BiasAdd:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n","sigmoid_out: Tensor(\"discriminator_a/Sigmoid:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n","inside discriminator model function\n","x is: Tensor(\"discriminator_a_1/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a_1/conv2d/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a_1/dropout/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a_1/conv2d_1/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a_1/dropout_1/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a_1/conv2d_2/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a_1/dropout_2/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a_1/Flatten/flatten/Reshape:0\", shape=(?, 65536), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_a_1/dense/Maximum:0\", shape=(?, 128), dtype=float32, device=/device:GPU:0)\n","logits: Tensor(\"discriminator_a_1/dense_1/BiasAdd:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n","sigmoid_out: Tensor(\"discriminator_a_1/Sigmoid:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n","inside discriminator model function\n","x is: Tensor(\"discriminator_b/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b/conv2d/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b/dropout/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b/conv2d_1/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b/dropout_1/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b/conv2d_2/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b/dropout_2/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b/Flatten/flatten/Reshape:0\", shape=(?, 65536), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b/dense/Maximum:0\", shape=(?, 128), dtype=float32, device=/device:GPU:0)\n","logits: Tensor(\"discriminator_b/dense_1/BiasAdd:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n","sigmoid_out: Tensor(\"discriminator_b/Sigmoid:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n","inside discriminator model function\n","x is: Tensor(\"discriminator_b_1/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b_1/conv2d/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b_1/dropout/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b_1/conv2d_1/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b_1/dropout_1/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b_1/conv2d_2/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b_1/dropout_2/Identity:0\", shape=(?, 32, 32, 64), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b_1/Flatten/flatten/Reshape:0\", shape=(?, 65536), dtype=float32, device=/device:GPU:0)\n","x is: Tensor(\"discriminator_b_1/dense/Maximum:0\", shape=(?, 128), dtype=float32, device=/device:GPU:0)\n","logits: Tensor(\"discriminator_b_1/dense_1/BiasAdd:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n","sigmoid_out: Tensor(\"discriminator_b_1/Sigmoid:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.global_variables_initializer` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_local_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.local_variables_initializer` instead.\n","Epochs begin at 0\n","losses da,ga,db,gb saved as: (0.35902867, 0.019496407, 0.35446906, 0.5524135)\n","Epochs begin at 1\n","losses da,ga,db,gb saved as: (0.5143089, 3.3764663, 0.3913068, 0.28286365)\n","batch_a (100, 12288)\n","batch_b (100, 12288)\n","inside encoder\n","input_ is: Tensor(\"generator_1/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32)\n","x is: Tensor(\"generator_1/Reshape:0\", shape=(?, 64, 64, 3), dtype=float32)\n","x is: Tensor(\"generator_1/leaky_re_lu_12/LeakyRelu:0\", shape=(?, 32, 32, 128), dtype=float32)\n","x is: Tensor(\"generator_1/leaky_re_lu_13/LeakyRelu:0\", shape=(?, 16, 16, 256), dtype=float32)\n","x is: Tensor(\"generator_1/leaky_re_lu_14/LeakyRelu:0\", shape=(?, 8, 8, 512), dtype=float32)\n","x is: Tensor(\"generator_1/leaky_re_lu_15/LeakyRelu:0\", shape=(?, 4, 4, 1024), dtype=float32)\n","x is: Tensor(\"generator_1/dense_7/BiasAdd:0\", shape=(?, 1024), dtype=float32)\n","x is: Tensor(\"generator_1/dense_8/BiasAdd:0\", shape=(?, 16384), dtype=float32)\n","x is: Tensor(\"generator_1/reshape_2/Reshape:0\", shape=(?, 4, 4, 1024), dtype=float32)\n","x is: Tensor(\"generator_1/pixel_shuffler_8/Reshape_1:0\", shape=(?, 8, 8, 512), dtype=float32)\n","encoder_val Tensor(\"generator_1/pixel_shuffler_8/Reshape_1:0\", shape=(?, 8, 8, 512), dtype=float32)\n","inside decoder\n","input_ is: Tensor(\"dec_a_1/Reshape:0\", shape=(?, 8, 8, 512), dtype=float32)\n","x is: Tensor(\"dec_a_1/Reshape:0\", shape=(?, 8, 8, 512), dtype=float32)\n","x is: Tensor(\"dec_a_1/pixel_shuffler_9/Reshape_1:0\", shape=(?, 16, 16, 256), dtype=float32)\n","x is: Tensor(\"dec_a_1/pixel_shuffler_10/Reshape_1:0\", shape=(?, 32, 32, 128), dtype=float32)\n","x is: Tensor(\"dec_a_1/pixel_shuffler_11/Reshape_1:0\", shape=(?, 64, 64, 64), dtype=float32)\n","x is: Tensor(\"dec_a_1/conv2d_22/Sigmoid:0\", shape=(?, 64, 64, 3), dtype=float32)\n","x is: Tensor(\"dec_a_1/dense_9/BiasAdd:0\", shape=(?, 12288), dtype=float32)\n","x shape is: (?, 12288)\n","decoder_a_val Tensor(\"dec_a_1/Tanh:0\", shape=(?, 12288), dtype=float32)\n","feed_dict_decoder {<tf.Tensor 'input_fake:0' shape=(?, 12288) dtype=float32>: array([[-0.56078431, -0.64705882, -0.69411765, ...,  0.15294118,\n","        -0.2627451 , -0.40392157],\n","       [-0.56078431, -0.64705882, -0.69411765, ...,  0.08235294,\n","        -0.28627451, -0.44313725],\n","       [-0.56078431, -0.64705882, -0.69411765, ...,  0.1372549 ,\n","        -0.30980392, -0.44313725],\n","       ...,\n","       [-0.56078431, -0.63137255, -0.68627451, ...,  0.27843137,\n","        -0.04313725, -0.2       ],\n","       [-0.56078431, -0.64705882, -0.69411765, ...,  0.06666667,\n","        -0.30980392, -0.46666667],\n","       [-0.57647059, -0.64705882, -0.71764706, ...,  0.12156863,\n","        -0.23137255, -0.41176471]])}\n"],"name":"stdout"},{"output_type":"error","ename":"FailedPreconditionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value dec_a_1/dense_10/bias\n\t [[{{node dec_a_1/dense_10/bias/read}}]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-3251661ec5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m samples_a, samples_b, losses, g_vars, g_vars_a, g_vars_b = train(scaledimages_a, scaledimages_b, input_size, \n\u001b[0;32m---> 48\u001b[0;31m                 latent_size, g_hidden_size, d_hidden_size, alpha, smooth, learning_rate, batch_size, epochs, reuse)\n\u001b[0m","\u001b[0;32m<ipython-input-29-0e523319bba1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(imageset_a, imageset_b, input_size, latent_size, g_hidden_size, d_hidden_size, alpha, smooth, learning_rate, batch_size, epochs, reuse)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;31m#                 sess.run(tf.local_variables_initializer())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mgen_samples_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_a_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'samples_a saved as:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value dec_a_1/dense_10/bias\n\t [[node dec_a_1/dense_10/bias/read (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:402) ]]\n\nCaused by op 'dec_a_1/dense_10/bias/read', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-3251661ec5d5>\", line 48, in <module>\n    latent_size, g_hidden_size, d_hidden_size, alpha, smooth, learning_rate, batch_size, epochs, reuse)\n  File \"<ipython-input-29-0e523319bba1>\", line 208, in train\n    reuse=True, scope=generator_decoder_a_scope)\n  File \"<ipython-input-28-51eb2be9566a>\", line 65, in decoder\n    x = Dense(64 * 64 * 3)(x)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\", line 872, in build\n    constraint=self.bias_constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 252, in add_weight\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 402, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1557, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 81, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3890, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dec_a_1/dense_10/bias\n\t [[node dec_a_1/dense_10/bias/read (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:402) ]]\n"]}]},{"metadata":{"id":"KLuIkhzSQwrn","colab_type":"code","colab":{}},"cell_type":"code","source":["img = samples_a[-1]\n","# img = postprocess_image(flattendedimages_a, img.reshape(160,160,3))\n","img = postprocess_image(flattendedimages_a, img.reshape(64,64,3))\n","imgplot = plt.imshow(img)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g2ECqoHPQwrr","colab_type":"code","colab":{}},"cell_type":"code","source":["img = samples_b[-1]\n","# img = postprocess_image(flattendedimages_b, img.reshape(160,160,3))\n","img = postprocess_image(flattendedimages_b, img.reshape(64,64,3))\n","imgplot = plt.imshow(img)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LUG8iL8lTgJ4","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}