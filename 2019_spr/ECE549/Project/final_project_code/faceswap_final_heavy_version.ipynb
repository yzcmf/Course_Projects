{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"faceswap_final_version.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dyOXV2BtSYgk","colab_type":"text"},"source":["#Part1 - Environment Setting\n","This ipynb is written by own our code and the below google drive link can be used to view the whole product, which with many reference codes as well. \n","\n","https://drive.google.com/open?id=1IgrvNOUylA7Wt5h5uXTKk8MGPAnnHoLj\n","\n","The work directory of our own design GAN model code can be found\n","\n","https://drive.google.com/drive/folders/1uyBz5vGpafZhJqeefS86pgUJnjCtJ3rX?usp=sharing\n","\n","The final results can be found in the following, included trump to cage train videos in 60s,120s,180s, epochs 21000,30000; trump to cage test videos in in 60s,120s,180s,epochs 21000,30000 etc\n","\n","https://drive.google.com/drive/folders/1mYbEDYWwjW3ZKnS3IdMrjHCnRZkRb1mZ?usp=sharing\n"]},{"cell_type":"code","metadata":{"id":"kCmvy8uVpOHM","colab_type":"code","outputId":"4aeb76ee-0264-4ae7-8115-35d487782c48","executionInfo":{"status":"ok","timestamp":1557170671178,"user_tz":300,"elapsed":2098,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["%matplotlib inline\n","import platform\n","from tensorflow.python.client import device_lib\n","import pickle as pkl\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import random as rand\n","import glob\n","import os\n","import math\n","import time\n","from PIL import Image\n","import cv2\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RMfNTNnGwKj7","colab_type":"code","outputId":"9d1f0c9e-8602-4170-e1f6-c09e9832e5cc","executionInfo":{"status":"ok","timestamp":1557170671181,"user_tz":300,"elapsed":2072,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd '/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code'\n","%pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/gdrive/My Drive/UIUC/2019_spring/ECE549--computer vision/Project/code'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"1tx5KxylFyz0","colab_type":"code","outputId":"dc007406-cae4-4c8e-e6dd-0ef823013703","executionInfo":{"status":"ok","timestamp":1557170679045,"user_tz":300,"elapsed":9908,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":1159}},"source":["import keras.backend as K\n","from keras.models import Model,Sequential,model_from_json\n","from keras.layers import Input, Dense, Flatten, Reshape, Activation, Dropout\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import Conv2D\n","from keras.optimizers import Adam\n","import glob\n","from tqdm import tqdm\n","!git clone https://gitlab.uip6.com/github/faceswap.git\n","import faceswap\n","from faceswap.lib.model.layers import PixelShuffler\n","!pip install moviepy\n","!pip install keras_vggface\n","import imageio\n","imageio.plugins.ffmpeg.download()\n","from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["fatal: destination path 'faceswap' already exists and is not an empty directory.\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.6/dist-packages (0.2.3.5)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (4.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from moviepy) (1.16.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (4.4.0)\n","Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (2.4.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (4.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio<3.0,>=2.1.2->moviepy) (0.46)\n","Requirement already satisfied: keras_vggface in /usr/local/lib/python3.6/dist-packages (0.5)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.16.3)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.2.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.12.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (4.3.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.13.1)\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.2.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.8.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->keras_vggface) (0.46)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (0.33.1)\n","Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.13.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (0.2.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.0.7)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.1.0)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (0.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.15.0)\n","Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.13.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (3.7.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.0.9)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (0.7.1)\n","Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->keras_vggface) (2.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->keras_vggface) (0.15.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->keras_vggface) (3.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->keras_vggface) (41.0.1)\n","Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->keras_vggface) (5.2.0)\n","[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 6504864171125407722\n",", name: \"/device:XLA_CPU:0\"\n","device_type: \"XLA_CPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 838318638197015089\n","physical_device_desc: \"device: XLA_CPU device\"\n",", name: \"/device:XLA_GPU:0\"\n","device_type: \"XLA_GPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 4568460772123599811\n","physical_device_desc: \"device: XLA_GPU device\"\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 14800692839\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 2637588761371890763\n","physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n","]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pq9V6hHnSkLW","colab_type":"text"},"source":["# Part2 - Face Images Data Processing"]},{"cell_type":"markdown","metadata":{"id":"7hLH2KsdkRYv","colab_type":"text"},"source":["##part2-1  Get the face images via upload videos"]},{"cell_type":"code","metadata":{"id":"lYb-HVaYT8YN","colab_type":"code","colab":{}},"source":["# from google.colab import files\n","# # Upload source video\n","# source_video = files.upload()\n","\n","# for fn_source_video, _ in source_video.items():\n","#     print(fn_source_video)\n","# # Upload target video\n","# target_video = files.upload()\n","\n","# for fn_target_video, _ in target_video.items():\n","#     print(fn_target_video)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8LxlxAHUCPY","colab_type":"code","colab":{}},"source":["# %%capture\n","# !pip install moviepy\n","# !pip install keras_vggface\n","# import imageio\n","# imageio.plugins.ffmpeg.download()\n","# fd = MTCNNFaceDetector(sess=K.get_session(), model_path=\"./mtcnn_weights/\")\n","# !mkdir -p faceA/rgb\n","# !mkdir -p faceA/binary_mask\n","# !mkdir -p faceB/rgb\n","# !mkdir -p faceB/binary_mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fq3arLbLyLHO","colab_type":"code","colab":{}},"source":["# fn_source_video = 'trump_0.mp4'\n","# fn_target_video = 'cage_0.mp4' \n","# print(str(len(glob.glob(\"faceA/rgb/*.*\"))) + \" face(s) extracted from source video: \" + fn_source_video + \".\")\n","# print(str(len(glob.glob(\"faceB/rgb/*.*\"))) + \" face(s) extracted from target video: \" + fn_target_video + \".\")\n","# train_A = glob.glob(\"faceA/rgb/*.*\")\n","# train_B = glob.glob(\"faceB/rgb/*.*\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AiIH9KuTpRQD","colab_type":"code","outputId":"9666b9ca-96a6-4de9-a9bf-c60af64bdff0","executionInfo":{"status":"ok","timestamp":1557170681563,"user_tz":300,"elapsed":12357,"user":{"displayName":"Jack Chou","photoUrl":"https://lh6.googleusercontent.com/-UR81iVJFUy4/AAAAAAAAAAI/AAAAAAAAAA4/zMijsFLy750/s64/photo.jpg","userId":"03136045414856911714"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["MIN = 0.0\n","MAX = 255.0\n","IMAGE_EDGE = 64\n","IMAGE_DIM = (IMAGE_EDGE, IMAGE_EDGE, 3)\n","\n","def show_image(image):\n","  print(np.mean(image))\n","  print(image)\n","  imgplot = plt.imshow(image)\n","  plt.show()\n","\n","# Normalize image\n","def preprocess_image(original_image):\n","  processed_image = original_image - MIN\n","  processed_image = (processed_image / (MAX - MIN)) * 2.0\n","  processed_image = processed_image - 1.0\n","  return processed_image\n","\n","# Undo normalization\n","def postprocess_image(processed_image):\n","  unnormalized = processed_image + 1.0\n","  unnormalized = (unnormalized / 2.0) * (MAX - MIN)\n","  unnormalized = unnormalized + MIN\n","  return unnormalized.astype(int)\n","\n","# SETUP IMAGES HERE, Images is array [image number, size of image]\n","\n","fn_source_video = 'trump_0.mp4'\n","fn_target_video = 'cage_0.mp4' \n","print(str(len(glob.glob(\"faceA/rgb/*.*\"))) + \" face(s) extracted from source video: \" + fn_source_video + \".\")\n","print(str(len(glob.glob(\"faceB/rgb/*.*\"))) + \" face(s) extracted from target video: \" + fn_target_video + \".\")\n","train_A = glob.glob(\"faceA/rgb/*.*\")\n","train_B = glob.glob(\"faceB/rgb/*.*\")\n","images_a = np.array([np.array(Image.open(fname).resize(IMAGE_DIM[0:2])) for fname in train_A])\n","images_b = np.array([np.array(Image.open(fname).resize(IMAGE_DIM[0:2])) for fname in train_B])\n","scaled_a = preprocess_image(images_a)\n","scaled_b = preprocess_image(images_b)\n","\n","flattendedimages_a = np.array(scaled_a).reshape((scaled_a.shape[0], -1))\n","flattendedimages_b = np.array(scaled_b).reshape((scaled_b.shape[0], -1))\n","# Size of input images to discriminators (SIZE OF IMAGE)\n","assert(flattendedimages_a.shape[1] == flattendedimages_b.shape[1])\n","input_size = flattendedimages_a.shape[1]\n","\n","# Scale images to fit functions\n","scaledimages_a = flattendedimages_a.reshape((-1,)+IMAGE_DIM)\n","scaledimages_b = flattendedimages_b.reshape((-1,)+IMAGE_DIM)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["708 face(s) extracted from source video: trump_0.mp4.\n","647 face(s) extracted from target video: cage_0.mp4.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IZBMN6ArkZHe","colab_type":"text"},"source":["##part2-2  Plot and save fake face images"]},{"cell_type":"code","metadata":{"id":"SiRW9tJ5aQAm","colab_type":"code","colab":{}},"source":["def plot_generated_images(epoch, generator, input_images, dim=(10,10), figsize=(10,10), label='a'):\n","    generated_images = generator.predict(input_images)\n","    generated_images = postprocess_image(generated_images.reshape(input_images.shape))\n","    plt.figure(figsize=figsize)\n","    for i in range(input_images.shape[0]):\n","        plt.subplot(dim[0], dim[1], i+1)\n","        plt.imshow(postprocess_image(input_images[i]), interpolation='nearest')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig('input_' + label + '_generated_image_%d.png' %epoch)\n","    for i in range(generated_images.shape[0]):\n","        plt.subplot(dim[0], dim[1], i+1)\n","        plt.imshow(generated_images[i], interpolation='nearest')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig('gan_' + label + '_generated_image_%d.png' %epoch)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GkrPTvZIS59L","colab_type":"text"},"source":["# Part3 - Build the GAN model "]},{"cell_type":"code","metadata":{"id":"Pz17odyapbCO","colab_type":"code","colab":{}},"source":["def create_encoder(latent_size, alpha=0.1):\n","  \n","  inputs = Input(shape=IMAGE_DIM)\n","  \n","  encoder = Reshape(IMAGE_DIM)(inputs)\n","  \n","  encoder = Conv2D(IMAGE_EDGE * 2, kernel_size=5, strides=2, padding='same')(encoder)\n","  encoder = LeakyReLU(alpha)(encoder)\n","  encoder = Conv2D(IMAGE_EDGE * 4, kernel_size=5, strides=2, padding='same')(encoder)\n","  encoder = LeakyReLU(alpha)(encoder)\n","  encoder = Conv2D(IMAGE_EDGE * 8, kernel_size=5, strides=2, padding='same')(encoder)\n","  encoder = LeakyReLU(alpha)(encoder)\n","  encoder = Conv2D(IMAGE_EDGE * 16, kernel_size=5, strides=2, padding='same')(encoder)\n","  encoder = LeakyReLU(alpha)(encoder)\n","\n","  encoder = Flatten()(encoder)\n","  encoder = Dense(latent_size)(encoder)\n","  encoder = Dense(4 * 4 * 1024)(encoder)\n","  encoder = Reshape((4, 4, 1024))(encoder)\n","  \n","  encoder = Conv2D(512 * 4, kernel_size=3, padding='same')(encoder)\n","  encoder = LeakyReLU(alpha)(encoder)\n","  encoder = PixelShuffler()(encoder)\n","  \n","  model = Model(inputs=[inputs],outputs=[encoder])\n","  \n","  return model\n","\n","def create_decoder(out_dim, alpha=0.1):\n","  \n","  inputs = Input(shape=(4, 4, 512 * 4))\n","  \n","  decoder = Reshape((8, 8, 512))(inputs)\n","\n","  decoder = Conv2D(256 * 4, kernel_size=3, padding='same')(decoder)\n","  decoder = LeakyReLU(alpha)(decoder)\n","  decoder = PixelShuffler()(decoder)\n","  \n","  decoder = Conv2D(128 * 4, kernel_size=3, padding='same')(decoder)\n","  decoder = LeakyReLU(alpha)(decoder)\n","  decoder = PixelShuffler()(decoder)\n","  \n","  decoder = Conv2D(64 * 4, kernel_size=3, padding='same')(decoder)\n","  decoder = LeakyReLU(alpha)(decoder)\n","  decoder = PixelShuffler()(decoder)\n","  \n","  decoder = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(decoder)\n","  \n","  decoder = Flatten()(decoder)\n","  decoder = Dense(out_dim)(decoder)\n","  decoder = Dense(64 * 64 * 3)(decoder)\n","  decoder = Activation('tanh')(decoder)\n","  decoder = Reshape(IMAGE_DIM)(decoder)\n","  \n","  model = Model(inputs=[inputs],outputs=[decoder])\n","  \n","  return model\n","\n","def create_discriminator(alpha=0.1, dropout=0.6):\n","  \n","  inputs = Input(shape=IMAGE_DIM)\n","  \n","  discriminator = Reshape(IMAGE_DIM)(inputs)\n","\n","  discriminator = Conv2D(IMAGE_EDGE, kernel_size=5, strides=2, padding='same')(discriminator)\n","  discriminator = LeakyReLU(alpha)(discriminator)\n","  discriminator = Dropout(dropout)(discriminator)\n","  \n","  discriminator = Conv2D(IMAGE_EDGE, kernel_size=5, strides=1, padding='same')(discriminator)\n","  discriminator = LeakyReLU(alpha)(discriminator)\n","  discriminator = Dropout(dropout)(discriminator)\n","  \n","  discriminator = Conv2D(IMAGE_EDGE, kernel_size=5, strides=1, padding='same')(discriminator)\n","  discriminator = LeakyReLU(alpha)(discriminator)\n","  discriminator = Dropout(dropout)(discriminator)\n","  \n","  discriminator = Flatten()(discriminator)\n","  discriminator = Dense(IMAGE_EDGE * 2)(discriminator)\n","  discriminator = LeakyReLU(alpha)(discriminator)\n","  discriminator = Dense(1)(discriminator)\n","  discriminator = Activation('sigmoid')(discriminator)\n","  \n","  model = Model(inputs=[inputs],outputs=[discriminator])\n","  \n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jD_1EmB-S-1o","colab_type":"text"},"source":[" # Part4 - Train function,loss and optimizers"]},{"cell_type":"code","metadata":{"id":"mNtyzDFXw2Fv","colab_type":"code","colab":{}},"source":["# Loss functions https://github.com/shaoanlu/faceswap-GAN/blob/87146b5995a41363f1d93e32c7c8338b78f80b11/networks/losses.py\n","def first_order(x, axis=1):\n","  img_nrows = x.shape[1]\n","  img_ncols = x.shape[2]\n","  if axis == 1:\n","    return K.abs(x[:, 0:img_nrows - 1, 0:img_ncols - 1, :] - x[:, 1:img_nrows, 0:img_ncols - 1, :])\n","  elif axis == 2:\n","    return K.abs(x[:, 0:img_nrows - 1, 0:img_ncols - 1, :] - x[:, 0:img_nrows - 1, 1:img_ncols, :])\n","  else:\n","    return None  \n","\n","def calc_loss(pred, target, loss='l2'):\n","  if loss.lower() == \"l2\":\n","    return K.mean(K.square(pred - target))\n","  elif loss.lower() == \"l1\":\n","    return K.mean(K.abs(pred - target))\n","  elif loss.lower() == \"cross_entropy\":\n","    return -K.mean(K.log(pred + K.epsilon())*target + K.log(1 - pred + K.epsilon())*(1 - target))\n","  else:\n","    raise ValueError(f'Recieve an unknown loss type: {loss}.')\n","    \n","def encoding_loss(y_true,y_pred):\n","  reconstruction_loss = calc_loss(y_pred, y_true, \"l1\")\n","  row_pred_consist = K.abs(y_pred[:, 0:64 - 1, 0:64 - 1, :] - y_pred[:, 1:64, 0:64 - 1, :])\n","  row_true_consist = K.abs(y_true[:, 0:64 - 1, 0:64 - 1, :] - y_true[:, 1:64, 0:64 - 1, :])\n","  col_pred_consist = K.abs(y_pred[:, 0:64 - 1, 0:64 - 1, :] - y_pred[:, 0:64 - 1, 1:64, :])\n","  col_true_consist = K.abs(y_true[:, 0:64 - 1, 0:64 - 1, :] - y_true[:, 0:64 - 1, 1:64, :])\n","  edge_loss_y = calc_loss(row_pred_consist, row_true_consist, \"l1\")\n","  edge_loss_x = calc_loss(col_pred_consist, col_true_consist, \"l1\")\n","  generator_loss = reconstruction_loss + edge_loss_y + edge_loss_x\n","  return generator_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtqenFQ-RtAx","colab_type":"code","colab":{}},"source":["def create_models():\n","  K.clear_session()\n","  \n","  # Creating GAN models\n","  inputs = Input(shape=IMAGE_DIM)\n","  \n","  gen_opt_a = Adam(lr=10e-3)\n","  gen_opt_b = Adam(lr=10e-3)\n","  encoder = create_encoder(1024)\n","  decoder_a = create_decoder(1024)\n","  decoder_b = create_decoder(1024)\n","  generator_a = Model(inputs, decoder_a(encoder(inputs)))\n","  generator_b = Model(inputs, decoder_b(encoder(inputs)))\n","  generator_a.compile(loss='binary_crossentropy', optimizer=gen_opt_a)\n","  generator_b.compile(loss='binary_crossentropy', optimizer=gen_opt_b)\n","  \n","  disc_opt_a = Adam(lr=10e-4)\n","  disc_opt_b = Adam(lr=10e-4)\n","  discriminator_a = create_discriminator()\n","  discriminator_b = create_discriminator()\n","  discriminator_a.compile(loss='binary_crossentropy', optimizer=disc_opt_a)\n","  discriminator_b.compile(loss='binary_crossentropy', optimizer=disc_opt_b)\n","  \n","  def make_trainable(net, val):\n","    net.trainable = val\n","    for l in net.layers:\n","        l.trainable = val\n","        \n","  make_trainable(discriminator_a, False)\n","  make_trainable(discriminator_b, False)\n","  fake_a = generator_a(inputs)\n","  fake_b = generator_b(inputs)\n","  validity_a = discriminator_a(fake_a)\n","  validity_b = discriminator_b(fake_b)\n","  GAN_a = Model(inputs, [fake_a, validity_a])\n","  GAN_b = Model(inputs, [fake_b, validity_b])\n","  GAN_a.compile(loss=[encoding_loss, 'binary_crossentropy'], optimizer=gen_opt_a, loss_weights=[1,0.1])\n","  GAN_b.compile(loss=[encoding_loss, 'binary_crossentropy'], optimizer=gen_opt_b, loss_weights=[1,0.1])\n","  return generator_a, generator_b, discriminator_a, discriminator_b, GAN_a, GAN_b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYq6pRp7f7X3","colab_type":"code","colab":{}},"source":["models_dir = \"./models\" # Path to saved model weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CLfbrCTybSsg","colab_type":"code","colab":{}},"source":["def training(generator_a, generator_b, discriminator_a, discriminator_b, GAN_a, GAN_b, imageset_a, imageset_b, past_epochs, epochs=100, batch_size=128):\n","  for e in range(1,epochs+1 ):\n","    np.random.shuffle(imageset_a)\n","    np.random.shuffle(imageset_b)\n","    for ii in range(min(imageset_a.shape[0]//batch_size, imageset_b.shape[0]//batch_size)):\n","      # image inputs\n","      batch_a = np.array(imageset_a[ii*batch_size:(ii+1)*batch_size])\n","      batch_b = np.array(imageset_b[ii*batch_size:(ii+1)*batch_size])\n","\n","      # Generate fake images from other image inputs\n","      generated_images_a = generator_a.predict(batch_b)\n","      generated_images_b = generator_b.predict(batch_a)\n","\n","      #Construct different batches of  real and fake data \n","      X_a = np.concatenate([batch_a, generated_images_a])\n","      X_b = np.concatenate([batch_b, generated_images_b])\n","      # Labels for generated and real data\n","      y_dis=np.zeros(2*batch_size)\n","      y_dis[:batch_size]=0.9\n","\n","      #Pre train discriminator on  fake and real data  before starting the gan. \n","      discriminator_a.trainable=True\n","      discriminator_a.train_on_batch(X_a, y_dis)\n","      discriminator_b.trainable=True\n","      discriminator_b.train_on_batch(X_b, y_dis)\n","\n","      y_gen = np.ones(batch_size)\n","\n","      # During the training of gan, \n","      # the weights of discriminator should be fixed. \n","      #We can enforce that by setting the trainable flag\n","      discriminator_a.trainable=False\n","      discriminator_b.trainable=False\n","\n","      #training  the GAN by alternating the training of the Discriminator \n","      #and training the chained GAN model with Discriminatorâ€™s weights freezed.\n","      GAN_a.train_on_batch(batch_b, [batch_a, y_gen])\n","      GAN_b.train_on_batch(batch_a, [batch_b, y_gen])\n","\n","    if e % 10 == 0:\n","      generator_a_json = generator_a.to_json()\n","      with open(models_dir + \"/generator_a.json\", \"w\") as json_file:\n","          json_file.write(generator_a_json)\n","      generator_a.save_weights(models_dir + \"/generator_a.h5\")\n","      generator_b_json = generator_b.to_json()\n","      with open(models_dir + \"/generator_b.json\", \"w\") as json_file:\n","          json_file.write(generator_b_json)\n","      generator_b.save_weights(models_dir + \"/generator_b.h5\")\n","      discriminator_a_json = discriminator_a.to_json()\n","      with open(models_dir + \"/discriminator_a.json\", \"w\") as json_file:\n","          json_file.write(discriminator_a_json)\n","      discriminator_a.save_weights(models_dir + \"/discriminator_a.h5\")\n","      discriminator_b_json = discriminator_b.to_json()\n","      with open(models_dir + \"/discriminator_b.json\", \"w\") as json_file:\n","          json_file.write(discriminator_b_json)\n","      discriminator_b.save_weights(models_dir + \"/discriminator_b.h5\")\n","      GAN_a_json = GAN_a.to_json()\n","      with open(models_dir + \"/GAN_a.json\", \"w\") as json_file:\n","          json_file.write(GAN_a_json)\n","      GAN_a.save_weights(models_dir + \"/GAN_a.h5\")\n","      GAN_b_json = GAN_b.to_json()\n","      with open(models_dir + \"/GAN_b.json\", \"w\") as json_file:\n","          json_file.write(GAN_b_json)\n","      GAN_b.save_weights(models_dir + \"/GAN_b.h5\")\n","      if e % 1000 == 0:\n","      # image inputs\n","       batch_a = np.array(imageset_a[0:batch_size])\n","       batch_b = np.array(imageset_b[0:batch_size])\n","       plot_generated_images(e + past_epochs, generator_a, batch_b, label='a')\n","       plot_generated_images(e + past_epochs, generator_b, batch_a, label='b')\n","       plot_generated_images(e + past_epochs, generator_a, batch_a, label='a_true')\n","       plot_generated_images(e + past_epochs, generator_b, batch_b, label='b_true')\n","  return generator_a, generator_b, discriminator_a, discriminator_b, GAN_a, GAN_b "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JjI6J0R0R1f5","colab_type":"code","colab":{}},"source":["reuse = False\n","if not reuse:\n","  generator_a, generator_b, discriminator_a, discriminator_b, GAN_a, GAN_b = create_models()\n","# else:\n","#   json_file = open(models_dir + '/generator_a.json', 'r')\n","#   loaded_model_json = json_file.read()\n","#   json_file.close()\n","#   generator_a = model_from_json(loaded_model_json)\n","#   generator_a.load_weights(models_dir + \"/generator_a.h5\")\n","  \n","#   json_file = open( models_dir + '/generator_b.json', 'r')\n","#   loaded_model_json = json_file.read()\n","#   json_file.close()\n","#   generator_b = model_from_json(loaded_model_json)\n","#   generator_b.load_weights(models_dir + \"/generator_b.h5\")\n","  \n","#   json_file = open(models_dir + '/discriminator_a.json', 'r')\n","#   loaded_model_json = json_file.read()\n","#   json_file.close()\n","#   discriminator_a = model_from_json(loaded_model_json)\n","#   discriminator_a.load_weights(models_dir + \"/discriminator_a.h5\")\n","  \n","#   json_file = open(models_dir + '/discriminator_b.json', 'r')\n","#   loaded_model_json = json_file.read()\n","#   json_file.close()\n","#   discriminator_b = model_from_json(loaded_model_json)\n","#   discriminator_b.load_weights(models_dir + \"/discriminator_b.h5\")\n","  \n","#   json_file = open(models_dir + '/GAN_a.json', 'r')\n","#   loaded_model_json = json_file.read()\n","#   json_file.close()\n","#   GAN_a = model_from_json(loaded_model_json)\n","#   GAN_a.load_weights(models_dir + \"/GAN_a.h5\")\n","  \n","#   json_file = open(models_dir + '/GAN_b.json', 'r')\n","#   loaded_model_json = json_file.read()\n","#   json_file.close()\n","#   GAN_b = model_from_json(loaded_model_json)\n","#   GAN_b.load_weights(models_dir + \"/GAN_b.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xg7H70OTNhH","colab_type":"text"},"source":["#Part5 - Face Image Training and Testing "]},{"cell_type":"code","metadata":{"id":"zQf-G4iyf9Pd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"c753b229-d8c5-49a7-fd0d-17e38a3d89c1"},"source":["epochs = 5000\n","batch_size = 100\n","past_epochs = 4000\n","generator_a, generator_b, discriminator_a, discriminator_b, GAN_a, GAN_b = training(generator_a, generator_b, discriminator_a, discriminator_b, GAN_a, GAN_b, scaledimages_a, scaledimages_b, past_epochs, epochs, batch_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py:98: MatplotlibDeprecationWarning: \n","Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  \"Adding an axes using the same arguments as a previous axes \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XtgxyeWkR-bL","colab_type":"code","colab":{}},"source":["global graph\n","graph = tf.get_default_graph() \n","with graph.as_default():\n","  random_input = np.random.rand(1,64,64,3)*2 - 1\n","  show_image(postprocess_image(generator_a.predict(random_input).reshape(64,64,3)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Dh_pam5WJG6","colab_type":"text"},"source":["# Part6 - Images convert back into Video\n","\n","Note: after train, this part can be used as either train videos or test videos\n","\n","This part can be done using reference code  as well, which will be more suitable code to convert images into videos\n","\n","All we need to do is give the h5 format weights to the gan model"]},{"cell_type":"markdown","metadata":{"id":"9qZu2zuAYFog","colab_type":"text"},"source":["## Part6-1 trained weights to face detector"]},{"cell_type":"code","metadata":{"id":"MXnG_EMiWOuY","colab_type":"code","colab":{}},"source":["from converter.video_converter import VideoConverter\n","from networks.faceswap_gan_model import FaceswapGANModel\n","global model, vggface\n","global train_batchA, train_batchB\n","# del model\n","# del vggface\n","# del train_batchA\n","# del train_batchB\n","tf.reset_default_graph()\n","K.clear_session()\n","# models_dir = \"./models\" # Path to saved model weights\n","# model = FaceswapGANModel(**arch_config)\n","# model.load_weights(path=models_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z4j2VzLUWeWk","colab_type":"code","colab":{}},"source":["# fd = MTCNNFaceDetector(sess=K.get_session(), model_path=\"./mtcnn_weights/\")\n","# vc = VideoConverter()\n","# vc.set_face_detector(fd)\n","# vc.set_gan_model(model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gUV3nG4Weh2","colab_type":"code","colab":{}},"source":["options = {\n","    # ===== Fixed =====\n","    \"use_smoothed_bbox\": True,\n","    \"use_kalman_filter\": True,\n","    \"use_auto_downscaling\": False,\n","    \"bbox_moving_avg_coef\": 0.65,\n","    \"min_face_area\": 35 * 35,\n","    \"IMAGE_SHAPE\": (64,64,3), #model.IMAGE_SHAPE\n","    # ===== Tunable =====\n","    \"kf_noise_coef\": 1e-3,\n","    \"use_color_correction\": \"hist_match\",\n","    \"detec_threshold\": 0.8,\n","    \"roi_coverage\": 0.9,\n","    \"enhance\": 0.,\n","    \"output_type\": 3,\n","    \"direction\": \"AtoB\", # ==================== This line determines the transform direction ====================\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1GIhShAXXkxu","colab_type":"text"},"source":["## Part6-2 input test video cases"]},{"cell_type":"code","metadata":{"id":"uasvtLkpWpXR","colab_type":"code","colab":{}},"source":["fn_source_video = 'trump_0.mp4'\n","fn_target_video = 'cage_0.mp4'\n","\n","# fn_source_video = 'cage_0.mp4'\n","# fn_target_video = 'trump_0.mp4'\n","\n","# fn_source_video = 'trump_1.mp4'\n","# fn_target_video = 'cage_1.mp4'\n","\n","# fn_source_video = 'trump_2.mp4'\n","# fn_target_video = 'cage_0.mp4'\n","\n","# fn_source_video = 'trump3_0.mp4'\n","# fn_target_video = 'cage_0.mp4'\n","\n","# fn_source_video = 'trump3_1.mp4'\n","# fn_target_video = 'cage_0.mp4'\n","\n","# fn_source_video = 'trump3_2.mp4'\n","# fn_target_video = 'cage_0.mp4'\n","\n","if options[\"direction\"] == \"AtoB\":\n","    input_fn = fn_source_video\n","    output_fn = \"OUTPUT_VIDEO_AtoB.mp4\"\n","elif options[\"direction\"] == \"BtoA\":\n","    input_fn = fn_target_video\n","    output_fn = \"OUTPUT_VIDEO_BtoA.mp4\"\n","\n","duration = None # None or a non-negative float tuple: (start_sec, end_sec). Duration of input video to be converted"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZVb3xSLWr9E","colab_type":"code","colab":{}},"source":["# vc.convert(input_fn=input_fn, output_fn=output_fn, options=options, duration=duration)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBr5zgVUXxmi","colab_type":"text"},"source":["## Part6-3 download output videos"]},{"cell_type":"code","metadata":{"id":"i7EUxBE4WzIE","colab_type":"code","colab":{}},"source":["# from google.colab import files\n","# if options[\"direction\"] == \"AtoB\":\n","#     files.download(\"OUTPUT_VIDEO_AtoB.mp4\")\n","# elif options[\"direction\"] == \"BtoA\":\n","#     files.download(\"OUTPUT_VIDEO_BtoA.mp4\")"],"execution_count":0,"outputs":[]}]}